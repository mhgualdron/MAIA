{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84e89b21e5d76d99e2f9ad14c5238f73",
     "grade": false,
     "grade_id": "cell-a76060540660d51c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Solución de MDPs por medio de iteración de valores\n",
    "\n",
    "Para este laboratorio utilizaremos el método de iteración de valores para solucionar el problema del motor del vehículo definido en el laboratorio de MDPs.\n",
    "\n",
    "Para ello utilizaremos el ambiente definido de los motores y construiremos el método de iteración de valores para resolverlo.\n",
    "\n",
    "## Ambiente\n",
    "\n",
    "Para recordar la definición del ambiente se realiza en la clase `MDP`. Esta clase recibe como parámetro una codificación de los estados, la dimensión del MDP dada por la cantidad de estados en el ambiente (dados como una tupla) y el estado inicial. \n",
    "\n",
    "La codificación del ambiente se define como una tabla de tamaño `estados x acciones` que para cada uno de los estados (filas) contiene una lista de tuplas con la información de la probabilidad asociada a la acción de la columna, el estado de llegada de ejecutar la acción y la recompensa asociada a la ejecución de la acción. La codificación de la tabla se muestra a continuación, donde los estados son `cold=0`, `warm=1` y `overheated=2`.\n",
    "\n",
    "![Encoding_table](table.png)\n",
    "\n",
    "Las dimensiones del tablero se almacenan en los astributos `nrows` y `ncols` de la clase `MDP`.\n",
    "Los valores codificados en el tablero se almacenarán en dos atributos de la clase, un atributo `transitions` que mantiene la probabilidad de transición de un estado a otro dada una acción como una lista de tuplas `(probability, state)` y un atributo `rewards` que mantiene la información las recompensas obtenida en el paso entre estados definido como una lista `(reward, state)`.\n",
    "\n",
    "Adicionalmente la clase tiene los atributos `initial_state` y `state` que corresponden al estado inicial, dado por parámetro, y el estado actual del agente en el ambiente, inicializado en el estado inicial.\n",
    "\n",
    "Note también que el estado `overheated` no tienen ninguna acción asociada a él y por lo tanto lo consideramos como un estado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6563f17928b3b9fb2925660c19a8dfd1",
     "grade": false,
     "grade_id": "cell-c6d646d279bdc96a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, table, dimensions, initial_state):\n",
    "        self.nrows, self.ncols = dimensions\n",
    "        self.transitions = [[0 for _ in range(self.ncols)] for _ in range(self.nrows)]\n",
    "        self.rewards = [[0 for _ in range(self.ncols)] for _ in range(self.nrows)]\n",
    "        \n",
    "        for i in range(self.nrows):\n",
    "            for j in range(self.ncols):\n",
    "                state = table[i][j]\n",
    "                if state is not None:\n",
    "                    list_transitions = []\n",
    "                    list_rewards = []\n",
    "                    for inf in state:\n",
    "                        p, s, r = inf\n",
    "                        list_transitions.append((p,s))\n",
    "                        list_rewards.append((r,s))\n",
    "                    self.transitions[i][j] = list_transitions\n",
    "                    self.rewards[i][j] = list_rewards\n",
    "                else:\n",
    "                    self.transitions[i][j] = None\n",
    "                    self.rewards[i][j] = None\n",
    "        self.initial_state = initial_state\n",
    "        self.state = self.initial_state\n",
    "    \n",
    "    def get_current_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        actions = ['slow', 'fast']\n",
    "        if state == 2:\n",
    "            actions = []\n",
    "        return actions\n",
    "    \n",
    "    def get_action_index(self, action):\n",
    "        actions = ['slow', 'fast']\n",
    "        index = 0\n",
    "        for a in actions:\n",
    "            if action == a:\n",
    "                return index\n",
    "            index += 1\n",
    "    \n",
    "    def get_possible_states(self, state, action):\n",
    "        action_index = self.get_action_index(action)\n",
    "        new_state = None\n",
    "        rewards=[]\n",
    "        states=[]\n",
    "        probabilities = []\n",
    "        for i in range(len(self.transitions[state][action_index])):\n",
    "            prob, new_state = self.transitions[state][action_index][i]\n",
    "            reward, _ = self.rewards[state][action_index][i]\n",
    "            probabilities.append(prob)\n",
    "            rewards.append(reward)\n",
    "            states.append(new_state)\n",
    "        return probabilities, rewards, states\n",
    "        \n",
    "    def simulate_action(self, state, action):\n",
    "        act = -1\n",
    "        if action == 'slow':\n",
    "            act = 0\n",
    "        else:\n",
    "            act = 1\n",
    "        start_state_action = self.transitions[state][act]\n",
    "        prob = 0\n",
    "        index = -1\n",
    "        new_state = -1\n",
    "        for op in start_state_action:\n",
    "            r = random.random() \n",
    "            if prob < r and r <= prob + op[0]:\n",
    "                new_state = op[1]\n",
    "            prob += op[0]\n",
    "            index += 1\n",
    "        return self.rewards[state][act][index][0], new_state\n",
    "    \n",
    "    def do_action(self, action):\n",
    "        act = -1\n",
    "        state = self.state\n",
    "        if action == 'slow':\n",
    "            act = 0\n",
    "        else:\n",
    "            act = 1\n",
    "        start_state_action = self.transitions[state][act]\n",
    "        prob = 0\n",
    "        index = -1\n",
    "        for op in start_state_action:\n",
    "            r = random.random() \n",
    "            if prob < r and r <= prob + op[0]:\n",
    "                self.state = op[1]\n",
    "            prob += op[0]\n",
    "            index += 1\n",
    "        return self.rewards[state][act][index][0], self.state\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.initial_state\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        return self.get_possible_actions(state) == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a2e269af191d6bb478614efa1c239fe",
     "grade": false,
     "grade_id": "cell-3851ea963f158fef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Creación del agente \n",
    "\n",
    "#### 1. definición de la clase de iteración de valores\n",
    "\n",
    "Implemente la clase `ValueIteration` que define cuatro atributos:\n",
    "\n",
    "- `mdp` que corresponde al MDP a resolver.\n",
    "- `discount` que corresponde al factor de decuento a utilizar.\n",
    "- `iterations` que corresponde a el número de iteraciones a realizar\n",
    "- `values` que corresponde a un mapa con los valores calculados para los estados del MDP. Los estados se definen como una tupla de los valores posibles del estado. Como simil, en el caso de Gridworld la tupla es la posición de cada casilla. Inicialmente definiremos el mapa como un mapa vacío, el cual poblaremos con los estados descubiertos durante cada iteración.\n",
    "\n",
    "Al momento de crear la clase `ValueIteration`, esta debe recibir por parámetro: la definición del MDP `MDP`, el valor de descuento `discount` (inicializado por defecto en 0.9 si no se pasa ningún valor) y la cantidad de iteraciones a ejecutar `iterations` (con un valor de 30 por defecto, si no se pasa )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53d59a6a6840fbd284bb5a72bb10a44a",
     "grade": false,
     "grade_id": "cell-10fb6b0ecbc7d12a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ValueIteration():    \n",
    "    def __init__(self, mdp, discount = 0.9, iterations = 30):\n",
    "        self.mdp = mdp\n",
    "        self.discount = discount\n",
    "        self.iterations = iterations\n",
    "        self.values = {}\n",
    "    \n",
    "    def get_value(self, state):\n",
    "        return self.values.get(state, 0)\n",
    "\n",
    "    def compute_new_value(self, state):\n",
    "        if self.mdp.is_terminal(state):\n",
    "            return 0 \n",
    "        \n",
    "        actions = self.mdp.get_possible_actions(state)\n",
    "        if not actions:\n",
    "            return 0\n",
    "        \n",
    "        action_values = []\n",
    "        for a in actions:\n",
    "            prob, rewards, next_states = self.mdp.get_possible_states(state, a)\n",
    "            value_sum = 0\n",
    "            for p, r, s_prime in zip(prob, rewards, next_states):\n",
    "                value_sum += p * (r + self.discount * self.get_value(s_prime))\n",
    "            action_values.append(value_sum)\n",
    "        return max(action_values) if action_values else 0\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if self.mdp.is_terminal(state):\n",
    "            return None\n",
    "        actions = self.mdp.get_possible_actions(state)\n",
    "        if not actions:\n",
    "            return None\n",
    "\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        for a in actions:\n",
    "            prob, rewards, next_states = self.mdp.get_possible_states(state, a)\n",
    "            value_sum = 0\n",
    "            for p, r, s_prime in zip(prob, rewards, next_states):\n",
    "                value_sum += p * (self.discount * self.get_value(s_prime))\n",
    "            if value_sum > best_value:\n",
    "                best_value = value_sum\n",
    "                best_action = a\n",
    "        return best_action\n",
    "        \n",
    "    def run_value_iteration(self):\n",
    "        for _ in range(self.iterations): \n",
    "            new_values = {}\n",
    "            for state in range(self.mdp.nrows):\n",
    "                new_values[state] = self.compute_new_value(state)\n",
    "            self.values = new_values\n",
    "\n",
    "class ValueIteration(ValueIteration):\n",
    "    def get_policy(self) -> dict:\n",
    "            self.run_value_iteration()\n",
    "            policy = {}\n",
    "            for state in range(self.mdp.nrows):\n",
    "                policy[state] = self.get_action(state)\n",
    "            return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del ambiente y creación del MDP\n",
    "board = [[[(1,0,1)], [(0.5, 0, 2), (0.5,1,2)]],\n",
    "         [[(0.5, 0, 1), (0.5,1,1)], [(1, 2, -10)]],\n",
    "         [None, None]]\n",
    "\n",
    "env = MDP(board, (3,2), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1b3dc258350d922cac551b096c3b816",
     "grade": true,
     "grade_id": "cell-05feea6b16200980",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas estructura del agente\n",
    "a = ValueIteration(env, 0.1, 10)\n",
    "\n",
    "try:\n",
    "    a.mdp\n",
    "    assert type(a.mdp) is MDP, \"El tipo del mdp debe ser MDP (el tipo de la clase)\"\n",
    "except:\n",
    "    print(\"El atributo mdp no está definido\")\n",
    "try:\n",
    "    a.discount\n",
    "    assert type(a.discount) is float, \"El tipo del factor de descuento debe ser float\"\n",
    "    assert 0 < a.discount and a.discount <=1, \"El factor de descuento debe ser un valor entre 0 (excluido) y 1 (incluido)\"\n",
    "except:\n",
    "    print(\"El atributo discount no está definido\")\n",
    "try:\n",
    "    a.iterations\n",
    "    assert type(a.iterations) is int, \"El tipo de la cantidad de iteraciones debe ser entero\"\n",
    "except:\n",
    "    print(\"El atributo iterations no está definido\")\n",
    "\n",
    "try:\n",
    "    a.values\n",
    "    assert type(a.values) is dict or type(a.values) is dict[int,float], \"El tipo del mapa de valeres debe ser dict (el tipo de los mapas en python). El mapa puede estar isntanciado en su llave y valor si fue inicializado previamente\"\n",
    "except:\n",
    "    print(\"El atributo values no está definido\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91ead88dee9cf098375a34af984c4a39",
     "grade": false,
     "grade_id": "cell-81a9d79dab9f5dec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2. Valores de estados \n",
    "\n",
    "Defina la función `get_value` que recibe un estado (el entero representando el estado) como parámetro y retorna el valor correspondiente para dicho estado. Si el estado no ha sido visitado, la función debe retornar 0, el valor inicial para el estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88aab1044c4b7a91c992cf48f6842f3f",
     "grade": true,
     "grade_id": "cell-dd097e0fe173b818",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de get_value\n",
    "\n",
    "a = ValueIteration(env, 0.8, 10)\n",
    "\n",
    "try:\n",
    "    a.get_value\n",
    "except:\n",
    "    print(\"La función get_value no está definida\")\n",
    "\n",
    "assert type(a.get_value(0)) is int or type(a.get_value(0)) is float, \"La función debe retornar un valor entero o un flotante.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a26012ef285f0f94aac4acaf9af8cf5",
     "grade": false,
     "grade_id": "cell-ecde5f76d16995e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 3. Cálculo de valor \n",
    "\n",
    "Defina la función `compute_new_value` que recibe un estado (entero en este caso) y calcula el nuevo valor para el estado siguiendo la fórmula de iteración de valores. Esta función calcula el nuevo valor para el estado actual del agente (`s`) a partir de las acciones, las recompensas y los valores de los posibles estados de llegada (`s'`) de acuerdo a la función de transición (o ruido). El nuevo valor calculado debe ser el valor máximo de todas las posibles acciones en el estado de entrada.\n",
    "\n",
    "Notas:\n",
    "- El valor inicial de todos los estados debe ser 0.\n",
    "- Para poder realizar este cálculo requerimos la nueva función del ambiente `get_possible_states`. La nueva función recibe el nombre de la acción a ejecutar para el estado actual del ambiente y retorna una lista con las probabilidades de transiciones entre los estados, una lista de recompensas y una lista de los estados de llegada para la acción desde el estado actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4439f745c476d77e44388ce8d8fab4a9",
     "grade": true,
     "grade_id": "cell-0090e5ec1f337910",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de compute_new_value\n",
    "\n",
    "a = ValueIteration(env, 0.8, 10)\n",
    "\n",
    "try:\n",
    "    a.compute_new_value\n",
    "except:\n",
    "    print(\"La función compute_new_value no está definida\")\n",
    "\n",
    "assert type(a.compute_new_value(0)) is int or type(a.compute_new_value(0)) is float, \"La función compute_new_value debe retornar un valor de tipo entero o flotante\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43e579511bfff3898ab9acf9751c364f",
     "grade": false,
     "grade_id": "cell-02e2e3e6dcac064f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 4. Acción a ejecutar \n",
    "\n",
    "Defina la función `get_action` que retorna la acción a tomar para un estado dado por parámetro, de acuerdo a los valores de los estados aledaños. Esta función debe observar el posible valor a obtener para cada una de sus acciones posibles y retornar aquella acción (su nombre) que lleva al mejor valor.\n",
    "Tenga en cuenta que la evaluación de las acciones no debe modificar el valor actual del estado, o tener un efecto visible sobre el ambiente o el agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1039ee05fabb4393ee8d7091c3f5937b",
     "grade": true,
     "grade_id": "cell-7ce9b8820afe56ea",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de get_action\n",
    "\n",
    "a = ValueIteration(env, 0.8, 50)\n",
    "\n",
    "try:\n",
    "    a.get_action\n",
    "except:\n",
    "    print(\"La función get_action no está definida\")\n",
    "\n",
    "assert type(a.get_action(0)) is str, \"La función get_action debe retornar el nombre de una acción tipo string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a9c6344b48f3a34b86729c4f570d78f",
     "grade": false,
     "grade_id": "cell-6631b8b1a1e70ed2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Preguntas Adicionales\n",
    "\n",
    "Calcule la política óptima para el agente. Para cumplir esta tarea debe definir una función, que ejecute la iteración de valores y retorne un diccionario donde para cada estado se retorna la acción óptima a ejecutar.\n",
    "\n",
    "¿Los valores convergen, porqué o porqué no?\n",
    "¿Hay alguna diferencia si comienza desde el estado frio, caliente, o sobrecalentado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print('ok')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
