{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85dcd102a3fdb3e5988c564bda8846c2",
     "grade": false,
     "grade_id": "cell-95d1a4259c54b654",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Solución de MDPs por medio de iteración de políticas\n",
    "\n",
    "Para este laboratorio utilizaremos el método de iteración de políticas para solucionar el problema del motor del vehículo definido en el laboratorio de MDPs.\n",
    "\n",
    "Para ello utilizaremos el ambiente definido de los motores y construiremos el método de iteración de políticas para resolverlo.\n",
    "\n",
    "## Ambiente\n",
    "\n",
    "Para recordar la definición del ambiente se realiza en la clase `MDP`. Esta clase recibe como parámetro una codificación de los estados, la dimensión del MDP dada por la cantidad de estados en el ambiente (dados como una tupla) y el estado inicial. \n",
    "\n",
    "La codificación del ambiente se define como una tabla de tamaño `estados x acciones` que para cada uno de los estados (filas) contiene una lista de tuplas con la información de la probabilidad asociada a la acción de la columna, el estado de llegada de ejecutar la acción y la recompensa asociada a la ejecución de la acción. La codificación de la tabla se muestra a continuación, donde los estados son `cold=0`, `warm=1` y `overheated=2`.\n",
    "\n",
    "![Encoding_table](table.png)\n",
    "\n",
    "Las dimensiones del tablero se almacenan en los astributos `nrows` y `ncols` de la clase `MDP`.\n",
    "Los valores codificados en el tablero se almacenarán en dos atributos de la clase, un atributo `transitions` que mantiene la probabilidad de transición de un estado a otro dada una acción como una lista de tuplas `(probability, state)` y un atributo `rewards` que mantiene la información las recompensas obtenida en el paso entre estados definido como una lista `(reward, state)`.\n",
    "\n",
    "Adicionalmente la clase tiene los atributos `initial_state` y `state` que corresponden al estado inicial, dado por parámetro, y el estado actual del agente en el ambiente, inicializado en el estado inicial.\n",
    "\n",
    "Note también que el estado `overheated` no tienen ninguna acción asociada a él y por lo tanto lo consideramos como un estado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b23e0d67b8cf02bc5ecb6f2552e6999",
     "grade": false,
     "grade_id": "cell-92af32cc5544794c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Ambiente de motores\n",
    "#Librerias requeridas\n",
    "import random\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, table, dimensions, initial_state):\n",
    "        self.nrows, self.ncols = dimensions\n",
    "        self.transitions = [[0 for _ in range(self.ncols)] for _ in range(self.nrows)]\n",
    "        self.rewards = [[0 for _ in range(self.ncols)] for _ in range(self.nrows)]\n",
    "        \n",
    "        for i in range(self.nrows):\n",
    "            for j in range(self.ncols):\n",
    "                state = table[i][j]\n",
    "                if state is not None:\n",
    "                    list_transitions = []\n",
    "                    list_rewards = []\n",
    "                    for inf in state:\n",
    "                        p, s, r = inf\n",
    "                        list_transitions.append((p,s))\n",
    "                        list_rewards.append((r,s))\n",
    "                    self.transitions[i][j] = list_transitions\n",
    "                    self.rewards[i][j] = list_rewards\n",
    "                else:\n",
    "                    self.transitions[i][j] = None\n",
    "                    self.rewards[i][j] = None\n",
    "        self.initial_state = initial_state\n",
    "        self.state = self.initial_state\n",
    "    \n",
    "    def get_current_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def get_possible_actions(self, state:int) -> list[str]:\n",
    "        actions = ['slow', 'fast']\n",
    "        if state == 2:\n",
    "            actions = []\n",
    "        return actions\n",
    "    \n",
    "    def get_action_index(self, action: str) -> int:\n",
    "        actions = ['slow', 'fast']\n",
    "        index = 0\n",
    "        for a in actions:\n",
    "            if action == a:\n",
    "                return index\n",
    "            index += 1\n",
    "    \n",
    "    def get_possible_states(self, state, action) -> tuple[list[float], list[int], list[int]]:\n",
    "        action_index = self.get_action_index(action)\n",
    "        new_state = None\n",
    "        rewards=[]\n",
    "        states=[]\n",
    "        probabilities = []\n",
    "        for i in range(len(self.transitions[state][action_index])):\n",
    "            prob, new_state = self.transitions[state][action_index][i]\n",
    "            reward, _ = self.rewards[state][action_index][i]\n",
    "            probabilities.append(prob)\n",
    "            rewards.append(reward)\n",
    "            states.append(new_state)\n",
    "        return probabilities, rewards, states\n",
    "        \n",
    "    def simulate_action(self, state, action):\n",
    "        act = -1\n",
    "        if action == 'slow':\n",
    "            act = 0\n",
    "        else:\n",
    "            act = 1\n",
    "        start_state_action = self.transitions[state][act]\n",
    "        prob = 0\n",
    "        index = -1\n",
    "        new_state = -1\n",
    "        for op in start_state_action:\n",
    "            r = random.random() \n",
    "            if prob < r and r <= prob + op[0]:\n",
    "                new_state = op[1]\n",
    "            prob += op[0]\n",
    "            index += 1\n",
    "        return self.rewards[state][act][index][0], new_state\n",
    "    \n",
    "    def do_action(self, action):\n",
    "        act = -1\n",
    "        state = self.state\n",
    "        if action == 'slow':\n",
    "            act = 0\n",
    "        else:\n",
    "            act = 1\n",
    "        start_state_action = self.transitions[state][act]\n",
    "        prob = 0\n",
    "        index = -1\n",
    "        for op in start_state_action:\n",
    "            r = random.random() \n",
    "            if prob < r and r <= prob + op[0]:\n",
    "                self.state = op[1]\n",
    "            prob += op[0]\n",
    "            index += 1\n",
    "        return self.rewards[state][act][index][0], self.state\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.initial_state\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        return self.get_possible_actions(state) == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02a7a25a922b94c2bc386a4df03c57ff",
     "grade": false,
     "grade_id": "cell-7631be86d36835a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Creación del agente \n",
    "\n",
    "#### 1. definición de la clase de iteración de políticas\n",
    "\n",
    "Implemente la clase `PolicyIteration` que define cinco atributos:\n",
    "\n",
    "- `mdp` que corresponde al MDP a resolver\n",
    "- `discount` que corresponde al factor de decuento a utilizar.\n",
    "- `iterations` que corresponde a el número de iteraciones a realizar\n",
    "- `values` que corresponde a un mapa con los valores calculados para los estados del MDP. Los estados se definen como una tupla de los valores posibles del estado. En el caso de Gridworld, la tupla es la posición de cada casilla. Inicialmente definiremos el mapa como un mapa vacío, el cual poblaremos con los estados descubiertos durante cada iteración.\n",
    "- `policy` que corresponde a un mapa con los nombres de las acciones a ejecutar para cada uno de los estados del ambiente. La política se debe inicializar con una acción aleatoria para cada estado. Si el estado no tiene acciones posibles, su política se debe inicializar en `None`. \n",
    "\n",
    "Al momento de crear la clase `PolicyIteration`, esta debe recibir por parámetro, la definición del MDP `MDP`, el valor de descuento `discount` (inicializado por defecto en 0.9 si no se pasa ningún valor) y la cantidad de iteraciones a ejecutar `iterations` (con un valor de 64 por defecto, si no se pasa ningún valor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5681eaa0e9743896208db5f6feacb05",
     "grade": false,
     "grade_id": "cell-61243e8a1a36a056",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class PolicyIteration():\n",
    "    def __init__(self, mdp: MDP, discount: float =0.9, iterations:int=64):\n",
    "        self.mdp = mdp\n",
    "        self.discount = discount\n",
    "        self.iterations = iterations\n",
    "        self.values = {} \n",
    "        self.policy = {} \n",
    "        \n",
    "        for s in range(self.mdp.nrows):\n",
    "            \n",
    "            actions = self.mdp.get_possible_actions(s)\n",
    "            if not actions:\n",
    "                self.policy[s] = None\n",
    "            else:\n",
    "                self.policy[s] = random.choice(actions)\n",
    "    \n",
    "    def set_policy(self, policy:list[str]):\n",
    "        self.policy = policy\n",
    "\n",
    "    def get_policy(self, state:int) -> str:\n",
    "        return self.policy.get(state, None)\n",
    "    \n",
    "    def get_value(self, state:int) -> float:\n",
    "        return self.values.get(state, 0.0)\n",
    "    \n",
    "    def value_evaluation(self, state:int, action:str) -> float:\n",
    "        \n",
    "        if self.mdp.is_terminal(state):\n",
    "            return 0.0\n",
    "            \n",
    "        probabilities, rewards, next_states = self.mdp.get_possible_states(state, action)\n",
    "        value_sum = 0.0\n",
    "        for p, r, s_prime in zip(probabilities, rewards, next_states):\n",
    "            value_sum += p * (r + self.discount * self.get_value(s_prime))\n",
    "        return value_sum\n",
    "        \n",
    "    def compute_new_value(self, state:int) -> tuple[float,str]:\n",
    "        \n",
    "        if self.mdp.is_terminal(state):\n",
    "            return 0.0, None \n",
    "\n",
    "        actions = self.mdp.get_possible_actions(state)\n",
    "        if not actions:\n",
    "            return 0.0, None\n",
    "\n",
    "        best_value = float('-inf')\n",
    "        best_action = None\n",
    "        \n",
    "        for a in actions:\n",
    "            current_value = self.value_evaluation(state, a)\n",
    "            if current_value > best_value:\n",
    "                best_value = current_value\n",
    "                best_action = a\n",
    "                \n",
    "        return best_value, best_action\n",
    "\n",
    "    def get_action(self, state:int) -> str:\n",
    "\n",
    "        if self.mdp.is_terminal(state):\n",
    "            return None\n",
    "        return self.policy.get(state)\n",
    "\n",
    "    def compute_new_action(self, state:int) -> str:\n",
    "        if self.mdp.is_terminal(state):\n",
    "            return None\n",
    "            \n",
    "        _ , best_action = self.compute_new_value(state)\n",
    "        return best_action\n",
    "\n",
    "    def policy_evaluation(self, new_policy:dict[int, str]) -> bool:\n",
    "        has_changed = False\n",
    "        for state in range(self.mdp.nrows):\n",
    "            if self.policy.get(state) != new_policy.get(state):\n",
    "                has_changed = True\n",
    "                break\n",
    "        return has_changed\n",
    "    \n",
    "    def policy_iteration(self) -> tuple[bool, dict[int,str]]:\n",
    "        tolerance = 1e-4 \n",
    "\n",
    "        for _ in range(100): \n",
    "            delta = 0\n",
    "            new_values = self.values.copy()\n",
    "            \n",
    "            for state in range(self.mdp.nrows):\n",
    "                old_value = self.get_value(state)\n",
    "                \n",
    "                if self.mdp.is_terminal(state):\n",
    "                    new_value = 0.0\n",
    "                else:\n",
    "                    action = self.get_policy(state) \n",
    "                    new_value = self.value_evaluation(state, action) \n",
    "                    \n",
    "                new_values[state] = new_value\n",
    "                delta = max(delta, abs(old_value - new_value))\n",
    "                \n",
    "            self.values = new_values\n",
    "            if delta < tolerance: \n",
    "                break\n",
    "\n",
    "        new_policy = {}\n",
    "        policy_stable = True\n",
    "        \n",
    "        for state in range(self.mdp.nrows):\n",
    "            old_action = self.get_policy(state)\n",
    "            new_action = self.compute_new_action(state) \n",
    "            \n",
    "            new_policy[state] = new_action\n",
    "            \n",
    "            if old_action != new_action:\n",
    "                policy_stable = False\n",
    "\n",
    "        converged = policy_stable\n",
    "        self.policy = new_policy\n",
    "        \n",
    "        return converged, self.policy\n",
    "\n",
    "    def run_policy_iteration(self) -> tuple[dict[float], dict[str]]:\n",
    "        done = False\n",
    "        i = 1\n",
    "        policy = {}\n",
    "        while not done and i <= self.iterations:\n",
    "            done, policy = self.policy_iteration()\n",
    "            i += 1\n",
    "        \n",
    "        if done:\n",
    "            print(f\"La política converge en {i-1} iteraciones\")\n",
    "            print(f\"Política : {policy}\")\n",
    "        else:\n",
    "            print(f\"La política no convergió después de {self.iterations} iteraciones\")\n",
    "            print(f\"Política final: {policy}\")\n",
    "            \n",
    "        return self.values, self.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ambiente general de prueba para todos los casos siguientes\n",
    "\n",
    "board = [[[(1,0,1)], [(0.5, 0, 2), (0.5,1,2)]],\n",
    "         [[(0.5, 0, 1), (0.5,1,1)], [(1, 2, -10)]],\n",
    "         [None, None]]\n",
    "\n",
    "env = MDP(board, (3,2), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f685ed8d1bd6e86d9cca99480be1d562",
     "grade": true,
     "grade_id": "cell-076d44492595eeee",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas estructura del agente\n",
    "\n",
    "a = PolicyIteration(env, 0.8, 10)\n",
    "\n",
    "try:\n",
    "    a.mdp\n",
    "    assert type(a.mdp) is MDP, \"El tipo del mdp debe ser MDP (el tipo de la clase)\"\n",
    "except:\n",
    "    print(\"El atributo mdp no está definido\")\n",
    "try:\n",
    "    a.discount\n",
    "    assert type(a.discount) is float, \"El tipo del factor de descuento debe ser float\"\n",
    "    assert 0 < a.discount and a.discount <=1, \"El factor de descuento debe ser un valor entre 0 (excluido) y 1 (incluido)\"\n",
    "except:\n",
    "    print(\"El atributo discount no está definido\")\n",
    "try:\n",
    "    a.iterations\n",
    "    assert type(a.iterations) is int, \"El tipo de la cantidad de iteraciones debe ser entero\"\n",
    "except:\n",
    "    print(\"El atributo iterations no está definido\")\n",
    "\n",
    "try:\n",
    "    a.values\n",
    "    assert type(a.values) is dict or type(a.values) is dict[int,float], \"El tipo del mapa de valeres debe ser dict (el tipo de los mapas en python). El mapa puede estar isntanciado en su llave y valor si fue inicializado previamente\"\n",
    "except:\n",
    "    print(\"El atributo values no está definido\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e64d5ab6ecfed2189d68e27b4869b0ec",
     "grade": false,
     "grade_id": "cell-7e9c3c0af0b464ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2. Política del agente\n",
    "\n",
    "Implemente la función `get_policy` que recibe un estado por parámetro y retorna el nombre de la acción (política) actual para dicho estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c046546d806fe5cbca05da9b7c9cbd54",
     "grade": true,
     "grade_id": "cell-a3f0c350a72c3b41",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas política\n",
    "\n",
    "a = PolicyIteration(env, 0.8, 10)\n",
    "\n",
    "try:\n",
    "    a.get_policy\n",
    "except:\n",
    "    print(\"La función get_policy no está definida\")\n",
    "\n",
    "assert type(a.get_policy(1)) is str, \"La función get_policy debe retornan un string con el nombre de la acción a ejecutar en el estado dado, o None si del estado no se puede ejecutar ninguna acción\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34da1b93243e3a40bd1100be9cc22956",
     "grade": false,
     "grade_id": "cell-0d1dfd8349ffd766",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 3. Valores de estados \n",
    "\n",
    "Defina la función `get_value` que recibe un estado (el entero representando el estado) como parámetro y retorna el valor correspondiente para dicho estado. Si el estado no ha sido visitado, la función debe retornar 0, el valor inicial para el estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "681dac012ad2154000aeeaa7c6f3c17a",
     "grade": true,
     "grade_id": "cell-070665537f19fe58",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de get_value\n",
    "\n",
    "a = PolicyIteration(env, 0.8, 10)\n",
    "\n",
    "\n",
    "try:\n",
    "    a.get_value\n",
    "except:\n",
    "    print(\"La función get_value no está definida\")\n",
    "\n",
    "assert type(a.get_value(0)) is int or type(a.get_value(0)) is float, f\"La función debe retornar un valor entero o un flotante\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c644db2454e4b37daa06967fecfc9e5",
     "grade": false,
     "grade_id": "cell-85b0a5ce579ed268",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 3. Evaluación de valores \n",
    "\n",
    "Defina la función `value_evaluation` que tiene como proposito calcular el valor de un estado (sin modificar el atributo de valores), dada una acción.\n",
    "Esta función recibe por parámetro un estado y una acción (el string del nombre) a ejecutar en el estado (entero en este caso) y retorna el valor calculado para dicho estado. \n",
    "\n",
    "Notas:\n",
    "- El valor por defecto de todos los estados debe ser 0.\n",
    "- Para poder realizar este cálculo requerimos la nueva función del ambiente `get_possible_states`. La nueva función recibe el nombre de la acción a ejecutar para el estado actual del ambiente y retorna una lista con las probabilidades de transiciones entre los estados, una lista de recompensas y una lista de los estados de llegada del estado actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc8a4538da0bcdf2e15d831e08efa41f",
     "grade": true,
     "grade_id": "cell-33f712394666172b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de value_evaluation\n",
    "\n",
    "a = PolicyIteration(env, 0.8, 10)\n",
    "\n",
    "try:\n",
    "    a.value_evaluation\n",
    "except:\n",
    "    print(\"La función value_evaluation no está definida\")\n",
    "\n",
    "assert type(a.value_evaluation(0, 'fast')) is int or type(a.value_evaluation(0,'fast')) is float, f\"La función value_evaluation debe retornar un valor de tipo entero o flotante, no {type(a.value_evaluation(0, 'fast'))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65ac2a839a68107942f8b1325a4a2faa",
     "grade": false,
     "grade_id": "cell-d2a6be582243b83a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 4. Cálculo de valor \n",
    "\n",
    "Defina la función `compute_new_value` que de forma similar a `value_evaluation` calcula el valor para el estado. La diferencia con la función anterior, es que `compute_new_value` calcula el valor máximo de todas las acciones posibles, dado un estado y no solo el valor de una única acción. \n",
    "Esta función recibe un estado como parámetro y calcula el nuevo valor para el estado siguiendo la fórmula de iteración de políticas. Esta función calcula el nuevo valor para el estado actual del agente (`s`) a partir de las acciones, las recompensas y los valores de los posibles estados de llegada (`s'`) de acuerdo a la función de transición (o ruido), obteniendo el valor máximo de todas las posibles acciones en el estado de entrada.\n",
    "\n",
    "Adicional al valor máximo, esta función debe retornar el nombre de la acción que nos lleva a ese máximo valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "847ed028c39c5a4f0e60f14d9ab5af5f",
     "grade": true,
     "grade_id": "cell-e3c9ea9ec965a4a8",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de compute_new_value\n",
    "\n",
    "a = PolicyIteration(env, 0.8, 50)\n",
    "\n",
    "try:\n",
    "    a.compute_new_value\n",
    "except:\n",
    "    print(\"La función compute_new_value no está definida\")\n",
    "\n",
    "assert type(a.compute_new_value(0)) is tuple, \"La función compute_new_value debe retornar una tupla (de tipo entero o flotante), no {type(a.compute_new_value(0))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b0728368f4ddf62d9ed876a525bcd3c",
     "grade": false,
     "grade_id": "cell-74cbf85b9d7d1301",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 5. Acción a ejecutar \n",
    "\n",
    "Defina la función `get_action` que retorna la acción a tomar para un estado dado por parámetro, de acuerdo a los valores de los estados aledaños. Esta función debe observar el posible valor a obtener para cada una de sus acciones posibles y retornar aquella acción (su nombre) que lleva al mejor valor.\n",
    "Tenga en cuenta que la evaluación de las acciones no debe modificar el valor actual del estado, o tener un efecto visible sobre el ambiente o el agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79de0aa1248b7abf89e1800af59f162c",
     "grade": true,
     "grade_id": "cell-289a68d5da2b8fa5",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de get_action\n",
    "\n",
    "a = PolicyIteration(env, 0.8, 10)\n",
    "\n",
    "\n",
    "try:\n",
    "    a.get_action\n",
    "except:\n",
    "    print(\"La función get_action no está definida\")\n",
    "\n",
    "assert type(a.get_action(0)) is str, \"La función get_action debe retornar el nombre de una acción tipo string\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7ecc051a10e0a70eeaef7aab19a9b43",
     "grade": false,
     "grade_id": "cell-21ce944b14e2b667",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 6. Calculo de acciones\n",
    "\n",
    "Implemente la función `compute_new_action` que retorna la nueva mejor acción a realizar luego de ejecutar el cálculo del nuevo valor para un estado. La función recibe un estado como parámetro y retorna la mejor acción a ejecutar para ese estado (actualizando la política).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03c3bba02dc9e93a2c4fd3872a5c46a2",
     "grade": true,
     "grade_id": "cell-3109672f07675f22",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de compute_new_action\n",
    "\n",
    "a = PolicyIteration(env, 0.8, 10)\n",
    "\n",
    "try:\n",
    "    a.compute_new_action\n",
    "except:\n",
    "    print(\"La función compute_new_action no está definida\")\n",
    "\n",
    "assert type(a.compute_new_action(0)) is str, \"La función compute_new_action debe retornar un str (o None si no hay acción posible)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebf983732c5c39076e03e494d0b8ac63",
     "grade": false,
     "grade_id": "cell-f4775c8463d8af59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 7. Evaluación de políticas \n",
    "\n",
    "Implemente la función `policy_evaluation` recibe la nueva política calculada como parámetro y evalua si existió algún cambio. En caso de tener un cambio con la política actual, la función retorna `True`, de lo contrario retorna `False`.\n",
    "\n",
    "Tenga en cuenta que la evaluación de la política se debe hacer sobre los estados del ambiente, en este caso `cold`, `hot`, `overheated`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4d5e8f43a3c8a84410cc82a2d319f85",
     "grade": true,
     "grade_id": "cell-72a4b3fe96abc7a8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de policy_evaluation\n",
    "\n",
    "a = PolicyIteration(env, 0.8, 50)\n",
    "\n",
    "try:\n",
    "    a.policy_evaluation\n",
    "except:\n",
    "    print(\"La función policy_evaluation no está definida\")\n",
    "\n",
    "assert type(a.policy_evaluation(a.policy)) is bool, \"La función policy_evaluation debe retornar un booleano\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46562ff9390742edaeb9aee47199f699",
     "grade": false,
     "grade_id": "cell-10dfe2cfa1f845b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 8. Iteración de políticas\n",
    "\n",
    "Implemente la función `policy_iteration` encargada de ejecutar la iteración de políticas en tres pasos. \n",
    "1. Primero se realiza el cálculo de los nuevos valores. Esto quiere decir que para cada uno de los estados del ambiente se calcula su nuevo valor utilizando la política actual (con la función `value_evaluation`).\n",
    "2. Segundo, para los nuevos valores se revisa la mejora política posible, calculando la mejor acción posible para cada uno de los estados (usando función `compute_new_action`). \n",
    "3. Tercero, se evalua la política calculada de las nuevas acciones en el paso anterior, actualizando la políticia si se encontraron mejores acciones. Si no hay una mejor acción, la función debe retornar la política calculada y un indicador (booleano) que hay convergencia para detener la iteración.\n",
    "\n",
    "La función debe retornar `True` si la política converge y `False` si no lo hace, junto con la política calculada "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "608a518e1ccc32f82a05beadd03ec4a5",
     "grade": true,
     "grade_id": "cell-b38c43e9564d2f49",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de policy_iteration\n",
    "\n",
    "a = PolicyIteration(env, 0.8, 10)\n",
    "\n",
    "### BEGIN TESTS\n",
    "board = [[[(1,0,1)], [(0.5, 0, 2), (0.5,1,2)]],\n",
    "         [[(0.5, 0, 1), (0.5,1,1)], [(1, 2, -10)]],\n",
    "         [None, None]]\n",
    "env = MDP(board, (3,2), 0)\n",
    "a = PolicyIteration(env, 0.8, 10)\n",
    "\n",
    "try:\n",
    "    a.policy_iteration\n",
    "except:\n",
    "    print(\"La función policy_iteration no está definida\")\n",
    "\n",
    "assert type(a.policy_iteration()) is tuple, \"La iteración de política debe retornar una tupla con un booleano y un mapa con la política\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bf09b8fab92ee9a8c7dca90483dd9f6",
     "grade": false,
     "grade_id": "cell-3cbf1a0acaf17282",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "#### Ejecución del agente \n",
    "\n",
    "Finalmente definimos la función `run_policy_iteration` que no recibe ningún parámatetro y ejecuta el algoritmo de solución del MDP.\n",
    "\n",
    "Esta función ejecuta el número de iteraciones dadas a la clase. Para cada una de las iteraciones se debe calcular el nuevo valor para cada uno de los estados del ambiente y la nueva política. \n",
    "\n",
    "Tenga en cuenta que el cálculo de los nuevos valores se debe realizar con los valores anteriores y únicamente se deben actualizar los valores cuando se actualicen los valores para todos los estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La política converge en 3 iteraciones\n",
      "Política : {0: 'fast', 1: 'slow', 2: None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({0: 15.499115145513922, 1: 14.499115145513922, 2: 0.0},\n",
       " {0: 'fast', 1: 'slow', 2: None})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pruebas de ejecución del agente\n",
    "\n",
    "a = PolicyIteration(env, 0.9, 100)\n",
    "\n",
    "a.run_policy_iteration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print('ok')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
