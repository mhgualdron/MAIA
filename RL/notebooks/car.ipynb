{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a1636e-3bee-42ac-afc5-60c11949fbc8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jp-MarkdownHeadingCollapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bcff5bb3f2e2e9b4e122e59a4e2a7c7",
     "grade": false,
     "grade_id": "cell-a61506ad831aeb46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "![apr](Banner.jpg)\n",
    "\n",
    "# Construcción de MDPs\n",
    "\n",
    "\n",
    "Para el taller de MDPs, vamos a construir el caso sencillo de un del ambiente de los estados del motor de un vehículo, como se muestra en la siguiente figura.\n",
    "\n",
    "![cars.png](cars_mdp.png)\n",
    "\n",
    "El ambiente contiene tres estados en los que se puede encontrar un vehículo en movimiento, frio (`cold` en azul), caliente (`hot` en rojo), o recalentado (`overheated` en gris). El vehículo tiene dos acciones posibles, ir rápido (`fast`) o ir lento (`slow`), las cuales se muestran como una etiqueta roja sobre las transiciones entre los estados. Asociado a cada una de las transiciones, también encontramos la recompensa asociada de ejecutar la acción.\n",
    "\n",
    "El trabajo de este taller consiste en construir un MDP que modele el ambiente mostrado en la figura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e812c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d5c96990674e8d06f14d5bb2120215a",
     "grade": false,
     "grade_id": "cell-ae02d5e231dc3eb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1. Estructura del ambiente\n",
    "\n",
    "Para definir el ambiente requerido utilizaremos una clase `MDP`. Esta clase recibe como parámetro una codificación de los estados, la dimensión del MDP dada por la cantidad de estados en el ambiente (dados como una tupla) y el estado inicial. \n",
    "\n",
    "La codificación del ambiente se puede realizar de varias formas. Por facilidad de uso más adelante en la ejecución de las funciones del ambiente, en este caso lo definiremos como una tabla de tamaño `estados x acciones` que para cada uno de los estados (filas) contiene una lista de tuplas con la información de la probabilidad asociada a la acción de la columna, el estado de llegada de ejecutar la acción y la recompensa asociada a la ejecución de la acción. La codificación de la tabla se muestra a continuación. Note que como una precondición, la definición de los estados debe ser correcta, enumerándolos como `cold=0`, `warm=1` y `overheated=2`.\n",
    "\n",
    "![Encoding_table](table.png)\n",
    "\n",
    "Las dimensiones del tablero se almacenan en los astributos `nrows` y `ncols` de la clase `MDP`.\n",
    "Los valores codificados en el tablero se almacenarán en dos atributos de la clase, un atributo `transitions` que mantiene la probabilidad de transición de un estado a otro dada una acción como una lista de tuplas `(probability, state)` y un atributo `rewards` que mantiene la información las recompensas obtenida en el paso entre estados definido como una lista `(reward, state)`.\n",
    "\n",
    "Adicionalmente la clase debe tener los atributos `initial_state` y `state` que corresponden al estado inicial, dado por parámetro, y el estado actual del agente en el ambiente, inicializado en el estado inicial.\n",
    "\n",
    "Note también que el estado `overheated` no tienen ninguna acción asociada a él y por lo tanto lo consideramos como un estado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4acc9d53-f1f4-4ec6-a5b5-ae9187cba2b7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d26e82160132f4ceac93fcabcc2c0b14",
     "grade": false,
     "grade_id": "cell-22200347aa420047",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Engine environment definition\n",
    "\n",
    "# Required libraries\n",
    "import random\n",
    "\n",
    "# Definition of the main class\n",
    "class MDP:\n",
    "    def __init__(self, table: list[list[tuple]], dimensions: tuple[int, int], initial_state: int):\n",
    "        self.model = table\n",
    "        self.nrows, self.ncols = dimensions\n",
    "        self.initial_state = initial_state\n",
    "        self.state = initial_state\n",
    "    \n",
    "        self.action_map = {\n",
    "            \"slow\": 0,\n",
    "            \"fast\": 1\n",
    "        }\n",
    "        \n",
    "        self.transitions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        for state_actions in table:\n",
    "            state_trans_list = []\n",
    "            state_rews_list = []\n",
    "            \n",
    "            for action_results in state_actions:\n",
    "                if action_results is None:\n",
    "                    state_trans_list.append(None)\n",
    "                    state_rews_list.append(None)\n",
    "                else:\n",
    "                    trans_tuples = []\n",
    "                    rews_tuples = []\n",
    "                    \n",
    "                    for (prob, next_state, reward) in action_results:\n",
    "                        trans_tuples.append((prob, next_state))\n",
    "                        rews_tuples.append((reward, next_state))\n",
    "                        \n",
    "                    state_trans_list.append(trans_tuples)\n",
    "                    state_rews_list.append(rews_tuples)\n",
    "                    \n",
    "            self.transitions.append(state_trans_list)\n",
    "            self.rewards.append(state_rews_list)\n",
    "        \n",
    "    def get_current_state(self) -> int:\n",
    "        return self.state\n",
    "    \n",
    "    def get_possible_actions(self, state: int) -> list[str]:\n",
    "        possible_actions = []\n",
    "        for action_name, action_idx in self.action_map.items():\n",
    "            if self.model[state][action_idx] is not None:\n",
    "                possible_actions.append(action_name)\n",
    "        return possible_actions\n",
    "        \n",
    "    def do_action(self, action: str) -> tuple[float, int]:\n",
    "        s = self.state\n",
    "        \n",
    "        if self.is_terminal():\n",
    "            return 0.0, s\n",
    "        \n",
    "        action_idx = self.action_map.get(action)\n",
    "        \n",
    "        if action_idx is None:\n",
    "            raise ValueError(f\"Unknown action: {action}\")\n",
    "        \n",
    "        results = self.model[s][action_idx]\n",
    "        \n",
    "        if results is None:\n",
    "            return 0.0, s\n",
    "        \n",
    "        probabilities = [out[0] for out in results]\n",
    "        next_states = [out[1] for out in results]\n",
    "        rewards = [out[2] for out in results]\n",
    "        \n",
    "        chosen_idx = random.choices(range(len(results)), weights=probabilities, k=1)[0]\n",
    "        \n",
    "        next_s = next_states[chosen_idx]\n",
    "        reward = rewards[chosen_idx]\n",
    "        \n",
    "        self.state = next_s\n",
    "        \n",
    "        return float(reward), next_s\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.initial_state\n",
    "    \n",
    "    def is_terminal(self) -> bool:\n",
    "        return len(self.get_possible_actions(self.state)) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c150242c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62bda5689aa3bfb5e1bca9932d2acf4d",
     "grade": true,
     "grade_id": "cell-8018062f76c7fb03",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas estructura del ambiente\n",
    "\n",
    "table = [[[(1,0,1)], [(0.5, 0, 2), (0.5,1,2)]],\n",
    "         [[(0.5, 0, 1), (0.5,1,1)], [(1, 2, -10)]],\n",
    "         [None, None]]\n",
    "\n",
    "env = MDP(table, (3,2), 0)\n",
    "\n",
    "try:\n",
    "    env.nrows\n",
    "except:\n",
    "    print(\"El atributo nrows no está definido\")\n",
    "try:\n",
    "    env.ncols\n",
    "except:\n",
    "    print(\"El atributo ncols no está definido\")\n",
    "try:\n",
    "    env.initial_state\n",
    "except:\n",
    "    print(\"El atributo initial_state no está definido\")\n",
    "try:\n",
    "    env.state\n",
    "except:\n",
    "    print(\"El atributo state no está definido\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd801388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c155ee-d567-41c9-af83-9226a429d3c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "930ba466ba19936a5b098f8be3417d99",
     "grade": false,
     "grade_id": "cell-da964bd84a299c31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2. Acciones del agente\n",
    "\n",
    "Defina la función `do_action` que ejecuta la acción tomada por el agente dentro del ambiente. Esta función recibe como parámetro la acción a ejecutar (el nombre de la acción a ejecutar dentro de las acciones posibles del agente) y retorna (una tupla con) el valor la recompensa del estado de llegada de la acción y el estado de llegada. \n",
    "\n",
    "Las acciones posibles del agente son `slow` y `fast` siguiendo la función de ruido dada por el MDP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1d864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5388f52",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9d9754d3f0d8d88cb43eed7b28b6ab4",
     "grade": true,
     "grade_id": "cell-9aad4d2ec90a470d",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de ejecución de las acciones\n",
    "\n",
    "table = [[[(1,0,1)], [(0.5, 0, 2), (0.5,1,2)]],\n",
    "         [[(0.5, 0, 1), (0.5,1,1)], [(1, 2, -10)]],\n",
    "         [None, None]]\n",
    "\n",
    "env = MDP(table, (3,2), 0)\n",
    "\n",
    "try:\n",
    "    env.do_action\n",
    "except:\n",
    "    print(\"La función do_action no está definida\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c6cec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9154f83f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ad550ef68dc29a99cc5e2e16d3cd4c9",
     "grade": false,
     "grade_id": "cell-d864f0766974e971",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 3. Estado actual\n",
    "\n",
    "Defina la función `get_current_state` que retorna el estado actual del agente. Esta función no recibe ningún parámetro y retorna el estado actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923cf73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66dbf84f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "014a799ab434cdb8790698da26ca7939",
     "grade": true,
     "grade_id": "cell-8c920f38c1368623",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas estado actual\n",
    "\n",
    "table = [[[(1,0,1)], [(0.5, 0, 2), (0.5,1,2)]],\n",
    "         [[(0.5, 0, 1), (0.5,1,1)], [(1, 2, -10)]],\n",
    "         [None, None]]\n",
    "\n",
    "env = MDP(table, (3,2), 0)\n",
    "\n",
    "try:\n",
    "    env.get_current_state\n",
    "except:\n",
    "    print(\"La función do_action no está definida\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "985f61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa8011c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7aaaa6d754704f5256c645225706ffc0",
     "grade": false,
     "grade_id": "cell-32fc596e5e4b9933",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 4. Obtener las acciones\n",
    "\n",
    "Defina la función `get_possible_actions` que recibe el estado actual del agente por parámetro y retorna una lista de las acciones válidas para el estado dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9900136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57fe0408",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86a3a35ef75dfd9e8181e0ef19984f55",
     "grade": true,
     "grade_id": "cell-bdd0d062b8f1859b",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de acciones posibles\n",
    "\n",
    "table = [[[(1,0,1)], [(0.5, 0, 2), (0.5,1,2)]],\n",
    "         [[(0.5, 0, 1), (0.5,1,1)], [(1, 2, -10)]],\n",
    "         [None, None]]\n",
    "\n",
    "env = MDP(table, (3,2), 0)\n",
    "\n",
    "try:\n",
    "    env.get_possible_actions\n",
    "except:\n",
    "    print(\"La función get_possible_actions no está definida\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d3e465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d07e0be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44bd46f3bfd228111b19306c255bc5e8",
     "grade": false,
     "grade_id": "cell-75a71d642969170e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "#### 5. Reinicializar el ambiente\n",
    "\n",
    "Defina la función `reset` que no recibe parámetros ni retorna ningún valor. El efecto de esta función es restablecer el ambiente a su estado inicial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b9804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7165df8c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae0bb942f76bf0a95e371ee9d6a84dea",
     "grade": true,
     "grade_id": "cell-232dc31e9084322a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de reset\n",
    "\n",
    "table = [[[(1,0,1)], [(0.5, 0, 2), (0.5,1,2)]],\n",
    "         [[(0.5, 0, 1), (0.5,1,1)], [(1, 2, -10)]],\n",
    "         [None, None]]\n",
    "\n",
    "env = MDP(table, (3,2), 0)\n",
    "\n",
    "try:\n",
    "    env.reset\n",
    "except:\n",
    "    print(\"La función reset no está definida\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c72ecb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62419601",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45f630161939720a30a8ec26c6b0e005",
     "grade": false,
     "grade_id": "cell-1c9d1f4ad60074f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "#### 6. Estados terminales\n",
    "\n",
    "Defina la función `is_terminal` que determina si el agente está en un estado final o no. En nuestro caso los estados finales o los estados de salida estarán determinados por las casillas con recompensa 1 o -1.\n",
    "Esta función no recibe parámetros y retorna un booleano determinando si el agente está en un estado final o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de20816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14eafe69",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0922d7b5c63cd983eafb611fdc7aeaf2",
     "grade": true,
     "grade_id": "cell-fcfdfbaeb99290c1",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas terminación\n",
    "\n",
    "table = [[[(1,0,1)], [(0.5, 0, 2), (0.5,1,2)]],\n",
    "         [[(0.5, 0, 1), (0.5,1,1)], [(1, 2, -10)]],\n",
    "         [None, None]]\n",
    "\n",
    "env = MDP(table, (3,2), 0)\n",
    "\n",
    "try:\n",
    "    env.is_terminal\n",
    "except:\n",
    "    print(\"La función is_terminal no está definida\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd7a6e1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb775010358c08a537afd62d23818cc4",
     "grade": false,
     "grade_id": "cell-52db1ae74027e75f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Ejecución\n",
    "\n",
    "Ahora vamos a ejecutar nuestro MDP.\n",
    "\n",
    "Para la ejecución del MDP vamos a ejecutar las todas las acciones hasta encontrar el estado terminal, observando los estados por los que pasa el agente y la recompensa asociada acumulada de toda la ejecución.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91923b09-c7e5-4b3e-bef1-b592d9735c71",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce0cdcdd214ac3a31afe8f7159fc49cc",
     "grade": true,
     "grade_id": "cell-e0742ccaef8993dd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando la acción fast desde el estado 0 al estado 0\n",
      "Ejecutando la acción fast desde el estado 0 al estado 0\n",
      "Ejecutando la acción fast desde el estado 0 al estado 0\n",
      "Ejecutando la acción slow desde el estado 0 al estado 0\n",
      "Ejecutando la acción fast desde el estado 0 al estado 0\n",
      "Ejecutando la acción fast desde el estado 0 al estado 0\n",
      "Ejecutando la acción slow desde el estado 0 al estado 0\n",
      "Ejecutando la acción fast desde el estado 0 al estado 0\n",
      "Ejecutando la acción fast desde el estado 0 al estado 1\n",
      "Ejecutando la acción slow desde el estado 1 al estado 0\n",
      "Ejecutando la acción fast desde el estado 0 al estado 0\n",
      "Ejecutando la acción fast desde el estado 0 al estado 0\n",
      "Ejecutando la acción slow desde el estado 0 al estado 0\n",
      "Ejecutando la acción fast desde el estado 0 al estado 0\n",
      "Ejecutando la acción slow desde el estado 0 al estado 0\n",
      "Ejecutando la acción fast desde el estado 0 al estado 0\n",
      "Ejecutando la acción fast desde el estado 0 al estado 1\n",
      "Ejecutando la acción fast desde el estado 1 al estado 2\n",
      "Ejecuto 18 pasos y obtuvo una recompensa de 19.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "table = [[[(1,0,1)], [(0.5, 0, 2), (0.5,1,2)]],\n",
    "         [[(0.5, 0, 1), (0.5,1,1)], [(1, 2, -10)]],\n",
    "         [None, None]]\n",
    "\n",
    "env = MDP(table, (3,2), 0)\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "while not done:\n",
    "    old_state = env.get_current_state()\n",
    "    action = random.choice(env.get_possible_actions(old_state))\n",
    "    reward, new_state = env.do_action(action)\n",
    "    total_reward += reward\n",
    "    print(f\"Ejecutando la acción {action} desde el estado {old_state} al estado {new_state}\")\n",
    "    steps += 1\n",
    "    done = env.is_terminal()\n",
    "print(f\"Ejecuto {steps} pasos y obtuvo una recompensa de {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e821b-bed3-4bdf-be15-12ec91f6d317",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ee895cc9afef58e124d1a8874ac49ee",
     "grade": false,
     "grade_id": "cell-960b691e20e4e6fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Preguntas de reflexión\n",
    "\n",
    "¿Qué puede concluir de la ejecución del MDP en cuanto a la cantidad de acciones que se ejejcutan? ¿Existe alguna diferencia? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72640e29",
   "metadata": {},
   "source": [
    "## Reflexiones acerca de la salida conseguida:\n",
    "\n",
    "1. El agent parte desde el estado inicial 0\n",
    "\n",
    "2. Tiene dos acciones posibles (0,1) con transiciones de probabilidad ya definidas\n",
    "\n",
    "3. Las transicoines con probabilidad < 1 hacen que varias veces el agent repita el mismo estado y eso se representqa en varias líneas repetias como por ejemplo:\n",
    "\n",
    "`Ejecutando la acción 0 desde el estado 0 al estado 0`\n",
    "\n",
    "`Ejecutando la acción 0 desde el estado 0 al estado 0`\n",
    "\n",
    "`Ejecutando la acción 0 desde el estado 0 al estado 0`\n",
    "\n",
    "`Ejecutando la acción 0 desde el estado 0 al estado 0`\n",
    "\n",
    "`Ejecutando la acción 0 desde el estado 0 al estado 0`\n",
    "\n",
    "`Ejecutando la acción 0 desde el estado 0 al estado 0`\n",
    "\n",
    "\n",
    "4. Tras varios intentos el agent finalmente logra llegar al estado 2 que es terminal y acumula una reward total de 26."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a92ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
