{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa555d22956b8be43d8c7a315f89db06",
     "grade": false,
     "grade_id": "cell-adc16c044dbbb5a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![banner](Banner.jpg)\n",
    "\n",
    "# Métodos de Monte Carlo\n",
    "\n",
    "En este ejercicio vamos a implementar una  solución para los problemas de aprendizaje por refuerzo basada en los métodos de Monte Carlo. \n",
    "\n",
    "Recuerde que el método de Monte Carlo consiste en la colección de muestras calculando los valores para la secuencia completa de los estados hasta el estado final del episodio. \n",
    "Una vez se han coleccionado \"suficientes\" muestras, el valor de los estados se toma como el valor promedio de las muestras sobre las cuales apareció el estado.\n",
    "\n",
    "## Ambiente\n",
    "\n",
    "En este laboratorio vamos a utilizar el método de Monte Carlo para construir un agente que resuleva el ambiente de la cruz que utilizamos en los videos.\n",
    "\n",
    "<img src=\"cruz.png\" width=\"200\"/>\n",
    "\n",
    "Como se muestra en la figura, el ambiente tiene cinco estados (`A` a `E`) configurados en una cruz. El ambiente tiene dos estados finales (`A` y `D`) y cuatro acciones posibles (`up`, `right`, `down`, `left`) para los estados `B`,`C`,`E` y una acción posible (`exit`) para los estados `A` y `D`. La codificación de las acciones para cada uno de los estados y su acción correspondiente se muestra en la tabla a continuación. La primera posición es la probabilidad, la segunda el estado de llegada y la tercera la recompensa.\n",
    "\n",
    "![Encoding_table](table.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c885fe0ae6b61973c31b7ef021ad0796",
     "grade": false,
     "grade_id": "cell-66d52e385dcd0644",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Definición del ambiente\n",
    "\n",
    "El ambiente se define dentro de la clase `MDP`, que recibe como parámetro el tablero con la codificación del MDP, las dimensiones del tablero (que se almacenan en los astributos `nrows` y `ncols`) y el estado inicial (que se almacena en el atributo `initial_state`).\n",
    "Los valores codificados en el tablero se almacenarán en dos atributos de la clase, un atributo `transitions` y otro `rewards`. El atributo `transitions` es un mapa (diccionario) con llaves de tipo `(estado, accion)`, cuyo valor para cada llave es la lista de `(probability, state)` posibles desde ese estado después de tomar esa acción. Por ejemplo, en la llave `('B','left')` debería existir una lista cuyo único elemento es `(1, 'B')`. El atributo `rewards` funciona de manera similar. Es un mapa donde para cada llave `(estado, accion)`, se tiene como valor una lista de los posibles `(reward, state)` que se pueden conseguir desde ese estado tomando esa acción. Por ejemplo, en la llave `('B','up')` debería existir una lista que tenga como elementos `(0, 'B')` y `(0, 'C')`. Las transiciones y recompensas que no existan se deben guardar como None, no como una lista vacía. \n",
    "\n",
    "La forma de representar acciones es como un string del siguiente conjunto `[\"up\", \"right\", \"down\", \"left\", \"exit\"]`. La forma de representar estados también es como un string parte del conjunto `['B', 'C', 'E', 'A', 'D']`.\n",
    "\n",
    "Adicionalmente la clase debe tener los atributos `initial_state` y `state` que corresponden al estado inicial, dado por parámetro, y el estado actual del agente en el ambiente, inicializado en el estado inicial.\n",
    "\n",
    "Note también que los estados `A` y `D` tienen una única acción asociada que lleva a ningun estado, marcandolos como estados finales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e58202b5eab95e69cf5e1be3b5bf989c",
     "grade": false,
     "grade_id": "cell-950eb1daaa29bad0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Librerias requeridas\n",
    "import numpy as np\n",
    "import random\n",
    "import sys # Usado para manejar errores\n",
    "\n",
    "#Definición de la clase principal\n",
    "class MDP:\n",
    "    def __init__(self, table: list[list[tuple[float,str,int]]], dimensions:tuple[int, int], initial_state:str):\n",
    "        self.nrows = dimensions[0]\n",
    "        self.ncols = dimensions[1]\n",
    "        self.initial_state = initial_state\n",
    "        self.state = initial_state\n",
    "        \n",
    "        self.transitions = {}\n",
    "        self.rewards = {}\n",
    "        \n",
    "        # Mapeo de índices a nombres de estados y acciones\n",
    "        states_map = ['B', 'C', 'E', 'A', 'D']\n",
    "        actions_map = ['up', 'right', 'down', 'left', 'exit']\n",
    "        \n",
    "        for i in range(len(states_map)):\n",
    "            for j in range(len(actions_map)):\n",
    "                current_state = states_map[i]\n",
    "                current_action = actions_map[j]\n",
    "                entry = table[i][j]\n",
    "                \n",
    "                key = (current_state, current_action)\n",
    "                \n",
    "                if entry is None:\n",
    "                    self.transitions[key] = None\n",
    "                    self.rewards[key] = None\n",
    "                else:\n",
    "                    t_list = []\n",
    "                    r_list = []\n",
    "                    for (prob, next_s, rew) in entry:\n",
    "                        t_list.append((prob, next_s))\n",
    "                        r_list.append((rew, next_s)) \n",
    "                    \n",
    "                    self.transitions[key] = t_list\n",
    "                    self.rewards[key] = r_list\n",
    "    \n",
    "    def get_action_index(self, action:str) -> int:\n",
    "        actions = ['up', 'right', 'down', 'left']\n",
    "        index = 0\n",
    "        for a in actions:\n",
    "            if action == a:\n",
    "                return index\n",
    "            index += 1\n",
    "        return -1\n",
    "\n",
    "    def get_states(self) -> list[str]:\n",
    "        states = set(())\n",
    "        for t in self.transitions:\n",
    "            states.add(t[0])\n",
    "        return list(states)\n",
    "\n",
    "    def get_possible_states(self, state:str, action:str) -> tuple[float,int,list[tuple[int,int]]]:\n",
    "        probability = []\n",
    "        rewards=[]\n",
    "        states=[]\n",
    "        \n",
    "\n",
    "        if (state, action) not in self.transitions or self.transitions[(state, action)] is None:\n",
    "             raise Exception(f\"Estado o acción inválida: {(state, action)}\")\n",
    "\n",
    "        for i in range(len(self.transitions[(state,action)])):\n",
    "            rewards.append(self.rewards[(state,action)][i][0])\n",
    "            probability.append(self.transitions[(state,action)][i][0])\n",
    "            states.append(self.transitions[(state,action)][i][1])\n",
    "        return probability, rewards, states\n",
    "    \n",
    "    def get_current_state(self) -> str:\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    def get_possible_actions(self, state:str) -> list[str]:\n",
    "\n",
    "        possible_actions = []\n",
    "        all_actions = ['up', 'right', 'down', 'left', 'exit']\n",
    "        \n",
    "        if state is None:\n",
    "            return []\n",
    "            \n",
    "        for action in all_actions:\n",
    "            if self.transitions.get((state, action)) is not None:\n",
    "                possible_actions.append(action)\n",
    "        return possible_actions\n",
    "    \n",
    "    def do_action(self, action:str) -> tuple[float, str]:\n",
    "        if action not in self.get_possible_actions(self.state):\n",
    "            raise Exception(f\"Acción '{action}' no es posible desde el estado '{self.state}'\")\n",
    "            \n",
    "        t_list = self.transitions[(self.state, action)]\n",
    "        r_list = self.rewards[(self.state, action)]\n",
    "        \n",
    "        probs = [t[0] for t in t_list]\n",
    "        next_states = [t[1] for t in t_list]\n",
    "        rewards = [r[0] for r in r_list]\n",
    "        indices = list(range(len(probs)))\n",
    "        chosen_index = random.choices(indices, weights=probs, k=1)[0]\n",
    "        \n",
    "        chosen_next_state = next_states[chosen_index]\n",
    "        chosen_reward = rewards[chosen_index]\n",
    "        self.state = chosen_next_state\n",
    "        return (chosen_reward, chosen_next_state)\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.initial_state\n",
    "    \n",
    "    def is_terminal(self, state:str) -> bool:\n",
    "        return state == 'A' or state == 'D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdcafdbbccc5cb102c7aebca5d56aa23",
     "grade": true,
     "grade_id": "cell-63d31178a81a88f3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Definición del ambiente\n",
    "\n",
    "table = [[[(0.8,'B',0), (0.2,'C',0)], [(0.7,'C',0), (0.3,'B',0)],[(0.7,'C',0), (0.3,'B',0)],[(1,'B',0)],None],\n",
    "         [[(0.7,'A',0), (0.1,'D',0), (0.2,'B',0)],[(0.7,'D',0), (0.1,'E',0), (0.2,'A',0)],[(0.7,'E',0), (0.1,'B',0), (0.2,'D',0)],[(0.7,'C',0), (0.1,'A',0), (0.2,'E',0)],None],\n",
    "         [[(0.7,'C',0), (0.3,'E',0)],[(0.8,'E',0),(0.2,'C',0)],[(1,'E',0)],[(0.9,'E',0), (0.1,'C',0)], None],\n",
    "         [None, None, None, None, [(1,None,-10)]],\n",
    "         [None, None, None, None, [(1,None,10)]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61f99a23f03b69cae9fb7f70f78b2958",
     "grade": true,
     "grade_id": "cell-8f1d5a7d25ee9e0a",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas estructura del ambiente.\n",
    "\n",
    "env = MDP(table, (5,5), 'B')\n",
    "\n",
    "try:\n",
    "    env.nrows\n",
    "    assert type(env.nrows) is int, \"Las dimensiones del ambiente deben ser enteras\"\n",
    "except:\n",
    "    raise Exception(\"El atributo nrows no está definido\")\n",
    "try:\n",
    "    env.ncols\n",
    "    assert type(env.ncols) is int, \"Las dimensiones del ambiente deben ser enteras\"\n",
    "except:\n",
    "    raise Exception(\"El atributo ncols no está definido\")\n",
    "try:\n",
    "    env.initial_state\n",
    "    assert type(env.initial_state) is str, \"El estado inicial debe corresponder al nombre de uno de los estados, un string\"\n",
    "except:\n",
    "    raise Exception(\"El atributo initial_state no está definido\")\n",
    "try:\n",
    "    env.state\n",
    "    assert type(env.state) is str, \"el estado del ambiente deben ser un string\"\n",
    "except:\n",
    "    raise Exception(\"El atributo state no está definido\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6441d7cae901aab8f5851d0d3efbea0",
     "grade": true,
     "grade_id": "cell-2c1efde9f366edfe",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas estructura del ambiente parte 2.\n",
    "\n",
    "env = MDP(table, (5,5), 'B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29deefbc201857c58343caf6550bf07a",
     "grade": true,
     "grade_id": "cell-fda738178c1097d3",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas estructura del ambiente parte 3. \n",
    "\n",
    "env = MDP(table, (5,5), 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c77b1416cd12f9ae8a80dc8a49dcd884",
     "grade": false,
     "grade_id": "cell-cd880559460c11ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Acciones del agente\n",
    "\n",
    "Defina la función `do_action` que ejecuta la acción tomada por el agente dentro del ambiente. Esta función recibe como parámetro el nombre de la acción a ejecutar y retorna una tupla con el valor de la recompensa del estado de llegada de la acción y el estado de llegada. \n",
    "\n",
    "Las acciones posibles del agente son `up`, `right`, `down` y `left` para los estados `B`, `C`, `E` y `exit` para los estados `A` y `D` siguiendo la función de transición dada por el MDP. El método debería lanzar una excepción cuando se intenta tomar una acción que no es posible. Por ejemplo, si intenta hacer `exit` desde el estado `B`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae03912f610fa838712e3528b5a0e3df",
     "grade": true,
     "grade_id": "cell-8acb28223d1fcff1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de ejecución de las acciones\n",
    "\n",
    "env = MDP(table, (5,5), 'B')\n",
    "\n",
    "try:\n",
    "    env.do_action\n",
    "except:\n",
    "    raise Exception(\"La función do_action no está definida\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "552d18f86eba4ac499417680c6d7d71b",
     "grade": true,
     "grade_id": "cell-1d7d8d34c9f56236",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de ejecución de las acciones parte 2\n",
    "\n",
    "env = MDP(table, (5,5), 'B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0dfba1d55802191ff4e69daedc1ff2f",
     "grade": true,
     "grade_id": "cell-8f039a765846ebb9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de ejecución de las acciones parte 3\n",
    "\n",
    "env = MDP(table, (5,5), 'B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRuebas adicionales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7cf895211c0eb3f7892113c75122833",
     "grade": false,
     "grade_id": "cell-c74aeecdf70448a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Estado actual\n",
    "\n",
    "Defina la función `get_current_state` que retorna el nombre del estado actual del agente. Esta función no recibe ningún parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98f1418eab9a113b115c0df1ccee9a4f",
     "grade": true,
     "grade_id": "cell-ce77642dd4133ecc",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas estado actual\n",
    "\n",
    "env = MDP(table, (5,5), 'B')\n",
    "\n",
    "try:\n",
    "    env.get_current_state\n",
    "    assert type(env.get_current_state()) is str, \"La función debe retornar el nombre del estado\"\n",
    "except:\n",
    "    raise Exception(\"La función do_action no está definida\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ba5767496d3645327339e9f4a528e9d",
     "grade": false,
     "grade_id": "cell-59193df31cf247b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Obtener las acciones\n",
    "\n",
    "Defina la función `get_possible_actions` que recibe el estado actual del agente por parámetro y retorna una lista de las acciones válidas para el estado dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2eb7e4daf08968e68a429e0dd3272f6",
     "grade": true,
     "grade_id": "cell-fc416048678d3a60",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de acciones posibles\n",
    "\n",
    "env = MDP(table, (5,5), 'B')\n",
    "\n",
    "try:\n",
    "    env.get_possible_actions\n",
    "    assert type(env.get_possible_actions('B')) is list, \"la función get_possible_actions debe ser una lista con las acciones disponibles para el estado\"\n",
    "except:\n",
    "    raise Exception(\"La función get_possible_actions no está definida\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "977ee3e82fceea3e7ddd94365302a0f9",
     "grade": true,
     "grade_id": "cell-cc766dbb7b9237af",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de acciones posibles parte 2\n",
    "\n",
    "env = MDP(table, (5,5), 'B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e2a4404ba425f497a0a0816a163c670",
     "grade": false,
     "grade_id": "cell-d3c5d838eaf617ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "#### Reinicializar el ambiente\n",
    "\n",
    "Defina la función `reset` que no recibe parámetros ni retorna ningún valor. El efecto de esta función es restablecer el ambiente a su estado inicial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0227ed3b040e6196b4a4a23897e1c405",
     "grade": true,
     "grade_id": "cell-226194925f1f0149",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas de reset\n",
    "\n",
    "env = MDP(table, (5,5), 'B')\n",
    "\n",
    "try:\n",
    "    env.reset\n",
    "except:\n",
    "    raise Exception(\"La función reset no está definida\")\n",
    "\n",
    "# BEGIN HIDDENT TESTS\n",
    "env = MDP(table, (5,5), 'B')\n",
    "\n",
    "env.state = 'E'\n",
    "env.reset()\n",
    "assert env.get_current_state() == env.initial_state, \"No se esta retornando al estado inicial. {env.get_current_state()} y no {env.initial_state}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1fe377c92925dda00deba38a2891a00f",
     "grade": false,
     "grade_id": "cell-77c48360ec115df1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "#### Estados terminales\n",
    "\n",
    "Defina la función `is_terminal` que recibe un estado com parámetro y determina si es un estado final o no. En nuestro caso los estados finales serán los estados `A` y `D`\n",
    "La función debe retornar un booleano que corresponde a si el estado pasado por parámetro es terminal o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93eb7bce49784b1046b731d1da6b047f",
     "grade": true,
     "grade_id": "cell-e8b3fbd7010421cd",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Pruebas terminación\n",
    "\n",
    "env = MDP(table, (5,5), 'B')\n",
    "\n",
    "try:\n",
    "    env.is_terminal\n",
    "    assert type(env.is_terminal('B')) is bool, \"La función debe retornar un booleano\"\n",
    "except:\n",
    "    print(\"La función is_terminal no está definida\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89a77f9035aecf9423daa3957ec53061",
     "grade": false,
     "grade_id": "cell-b93f383eafb9279e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Método de Monte Carlo\n",
    "\n",
    "Ahora construiremos un agente que utilice el método de Monte Carlo para resolver el MDP definido.\n",
    "\n",
    "El comportamiento del agente (de Monte Carlo) esta dado por dos momentos. El proceso de recolección de muestras y el proceso de explotación de las mismas, es decir, el cálculo de la política del agente. Usted debe implementar el comportamiento del agente dado que, ejecutando episodios como muestras, sea capaz de calcular los valores para los estados.\n",
    "\n",
    "#### 1. Creación del agente\n",
    "\n",
    "Implemente la classe `MCM` para solucionar el ambiente de la cruz. La clase debe tener los siguientes atributos:\n",
    "- El ambiente `env`, que se recibe por parámetro. La única interacción que se tendrá con el ambiente es para ejecutar acciones y recibir la retroalimentación correspondiente \n",
    "- El factor de descuento `discount`, que se pasa por parámetro. Si no se da este parámetro el valor por defecto es `0.9` \n",
    "- La cantidad de iteraciones `episodes` a ejecutar, que se pasa por parámetro. Si no se da este parámetro el valor por defecto es 1000\n",
    "- El threshold `threshold` de ejecución que pasa por parámetro. Determina cuándo dos números son lo suficientemente similares para afirmar que convergieron. Si no se pasa este parámetro, su valor por defecto de `0.00001`. \n",
    "- Los valores de los estados `values` que se definen como un mapa de parejas `{estado:valor}` almacenando los valores para cada uno de los estados. Como en casos anteriores inicializaremos este atributo como un mapa vacio (i.e., `{}`) que se llenará de acuerdo a los valores observados. \n",
    "- Las recompensas `rewards` que almacena el valor obtenido de la recompensa de ejecutar una acción para cada estado. El mapa de `rewards` como un mapa `{(estado,acción): valor}`. Al igual que en el caso anterior, este mapa se inicializa vácio (i.e., `{}`) y se supone que los valores iniciales son todos `0` si no se han definido antes de usarlos. \n",
    "- La política `policy`, que que lleva la política de las mejores acciones a ejecutar por el agente. Este atributo se define como un mapa de parejas `{estado:acción}`, inicializandolo vació. \n",
    "- La cantidad de muestras `samples` ejecutadas por el agente, inicializada en 0\n",
    "- La cantidad de episodios de exploración `explore` a ejecutar por el agente, inicializado en 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcff717a0eaf3d9d25a2e29b38d7b9c1",
     "grade": false,
     "grade_id": "cell-207f1b41a61f6afa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Librerias a importar\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class MCM():\n",
    "\n",
    "    def __init__(self, environment:MDP, discount:float = 0.9, iterations:int =1000, threshold:float =0.00001):\n",
    "        self.env = environment\n",
    "        self.discount = discount\n",
    "        self.episodes = iterations\n",
    "        self.threshold = threshold\n",
    "        self.values = {} \n",
    "        self.rewards = {} \n",
    "        self.policy = {}\n",
    "        self.samples = 0\n",
    "    \n",
    "    def get_values(self, state:str) -> int:\n",
    "        if self.values.get(state) == None:\n",
    "            return 0\n",
    "        return self.values.get(state)\n",
    "    \n",
    "    def generate_episode(self) -> list[tuple[str,str,int]]:\n",
    "    \n",
    "        self.env.reset() \n",
    "        episode_history = []\n",
    "        \n",
    "        while self.env.get_current_state() is not None:\n",
    "            current_state = self.env.get_current_state()\n",
    "            \n",
    "            possible_actions = self.env.get_possible_actions(current_state)\n",
    "            \n",
    "            if not possible_actions:\n",
    "                break\n",
    "                \n",
    "            chosen_action = random.choice(possible_actions)\n",
    "            (reward, next_state) = self.env.do_action(chosen_action)\n",
    "            episode_history.append((current_state, chosen_action, reward))\n",
    "            self.samples += 1\n",
    "            \n",
    "        return episode_history\n",
    "\n",
    "    def check_convergence(self, current_values:dict) -> bool:     \n",
    "        is_converged = True\n",
    "        all_states = set(self.values.keys()) | set(current_values.keys())\n",
    "\n",
    "        for state in all_states:\n",
    "            old_val = self.values.get(state, 0) \n",
    "            new_val = current_values.get(state, 0)\n",
    "            \n",
    "            if abs(new_val - old_val) > self.threshold:\n",
    "                is_converged = False\n",
    "                break\n",
    "        \n",
    "        if not is_converged:\n",
    "           \n",
    "            self.values = current_values.copy() \n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def calculate_rewards(self, episode:list[tuple[str,str,int]]) -> dict[list[int]]:\n",
    "        \n",
    "        returns_per_state = {}\n",
    "        G = 0 \n",
    "        visited_states = set()\n",
    "        for (state, action, reward) in reversed(episode):\n",
    "            G = reward + self.discount * G\n",
    "            \n",
    "            if state not in visited_states:\n",
    "                if state not in returns_per_state:\n",
    "                    returns_per_state[state] = []\n",
    "                returns_per_state[state].append(G)\n",
    "                visited_states.add(state)\n",
    "                \n",
    "        return returns_per_state\n",
    "        \n",
    "    def run(self) -> tuple[dict, dict]:\n",
    "        # Ejecuta el método de Monte Carlo por el número de episodios especificado.\n",
    "        values = {}\n",
    "        for i in range(self.episodes):\n",
    "            episode = self.generate_episode()\n",
    "            rewards = self.calculate_rewards(episode)\n",
    "            \n",
    "            # Actualiza las recompensas con las recompensas de cada estado\n",
    "            for state in rewards:\n",
    "                if state not in self.rewards:\n",
    "                    self.rewards[state] = []\n",
    "                self.rewards[state].extend(rewards[state])\n",
    "            \n",
    "            for state in rewards:\n",
    "                # El valor del estado esta dado por la media de las recompensas obtenidas para el estado durante el episodio\n",
    "                values[state] = np.mean(self.rewards[state])\n",
    "            if self.check_convergence(values):\n",
    "                print(\"Convergio en el episodio\",i)\n",
    "                break\n",
    "        \n",
    "        # La política para cada estado usando los valores actualizados\n",
    "        for state in self.env.get_states():  \n",
    "            possible_actions = self.env.get_possible_actions(state)\n",
    "            if len(possible_actions) > 0:\n",
    "                values = []\n",
    "                for action in possible_actions:\n",
    "                    prob, reward, next_state = self.env.get_possible_states(state, action)\n",
    "                    value = 0\n",
    "                    for i in range(len(prob)):\n",
    "                        previous_val = self.get_values(next_state[i]) if next_state[i] != None else 0\n",
    "                        value += prob[i]*(reward[i] + self.discount * previous_val)\n",
    "                    values.append(value)\n",
    "                best_action = possible_actions[np.argmax(values)]\n",
    "                self.policy[state] = best_action\n",
    "        \n",
    "        # Returna los valores calculados y la política\n",
    "        return self.values, self.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1130e353e1ed87e8e941991621c1e0a7",
     "grade": true,
     "grade_id": "cell-0b00a5795eefbdbf",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Escenarios de prueba\n",
    "\n",
    "table = [[[(0.8,'B',0), (0.2,'C',0)], [(0.7,'C',0), (0.3,'B',0)],[(0.7,'C',0), (0.3,'B',0)],[(1,'B',0)],None],\n",
    "         [[(0.7,'A',0), (0.1,'D',0), (0.2,'B',0)],[(0.7,'D',0), (0.1,'E',0), (0.2,'A',0)],[(0.7,'E',0), (0.1,'B',0), (0.2,'D',0)],[(0.7,'C',0), (0.1,'A',0), (0.2,'E',0)],None],\n",
    "         [[(0.7,'C',0), (0.3,'E',0)],[(0.8,'E',0),(0.2,'C',0)],[(1,'E',0)],[(0.9,'E',0), (0.1,'C',0)], None],\n",
    "         [None, None, None, None, [(1,None,-10)]],\n",
    "         [None, None, None, None, [(1,None,10)]]]\n",
    "\n",
    "environment = MDP(table, (5,5), 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03831f435fd8524f2297096aafbd7449",
     "grade": true,
     "grade_id": "cell-3748c437ec2f6974",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mcm = MCM(environment, discount=0.9, iterations=1000, threshold=0.00001)\n",
    "\n",
    "try:\n",
    "    mcm.env\n",
    "    assert type(mcm.env) is MDP, \"El ambiente debe estar definido como un MDP\"\n",
    "except:\n",
    "    raise Exception(\"El atributo env no está definido\")\n",
    "try:\n",
    "    mcm.discount\n",
    "    assert type(mcm.discount) is float, \"La taza de descuento debe ser un número flotante\"\n",
    "except:\n",
    "    raise Exception(\"La taza de descuento no esta definida como discount\")\n",
    "try:\n",
    "    mcm.episodes\n",
    "    assert type(mcm.episodes) is int, \"La cantidad de episodios debe ser un entero\"\n",
    "except:\n",
    "    raise Exception(\"El atributo episodes no está definido\")\n",
    "try:\n",
    "    mcm.threshold\n",
    "    assert type(mcm.threshold) is float, \"El threshold debe ser un número flotante\"\n",
    "except:\n",
    "    raise Exception(\"El atributo threshold no está definido\")\n",
    "try:\n",
    "    mcm.values\n",
    "    assert type(mcm.values) is dict, \"Los valores deben estar definidos como un mapa\"\n",
    "except:\n",
    "    raise Exception(\"El atributo values no está definido\")\n",
    "try:\n",
    "    mcm.rewards\n",
    "    assert type(mcm.rewards) is dict, \"Las recompensas deben estar definidas como un mapa\"\n",
    "except:\n",
    "    raise Exception(\"El atributo rewards no está definido\")\n",
    "try:\n",
    "    mcm.policy\n",
    "    assert type(mcm.policy) is dict, \"Las políticas deben estar definidas como un mapa\"\n",
    "except:\n",
    "    raise Exception(\"El atributo policy no está definido\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ba1127249ebca26c8b3a00e0d5728b6",
     "grade": false,
     "grade_id": "cell-3d902f8a8455ba15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2. Generar episodios\n",
    "\n",
    "Implemente la función `generate_episode` que, dado el agente, desde el estado actual, continuamente escoge una acción a realizar (aleatoriamente) y obtiene la recompensa correspondiente. Este proceso se repite hasta que se completa el episodio (se alcanza el objetivo, un estado final). La función debe retornar la secuencia de acciones como una lista (i.e., tuplas (estado, acción, recompensa) tomadas durante el episodio.\n",
    "\n",
    "La generación del episodio comienza desde el estado inicial del ambiente y utiliza una política aleatoria para escoger la acción en cada estado visitado hasta llegar al estado final. Para cada iteración durante el episodio se acumula el número de muestras tomadas en la variable `samples` (inicializada en 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e9e132cbe60feddb3e35ae86a59d081",
     "grade": true,
     "grade_id": "cell-15d2629e2196d656",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mcm = MCM(environment, discount=0.9, iterations=1000, threshold=0.00001)\n",
    "\n",
    "try:\n",
    "    mcm.generate_episode\n",
    "    assert type(mcm.generate_episode()) is list, \"El ambiente debe estar definido como un MDP\"\n",
    "except:\n",
    "    raise Exception(\"La función generate_episode no está definida\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a8be0f2636c42ab5f24b5eb6c2bacb90",
     "grade": false,
     "grade_id": "cell-8e6970e34f3e7067",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 3. Evaluar la convergencias\n",
    "\n",
    "Implemente la función de convergencia `check_convergence` que determina si los valores explorados convergen para las iteraciones realizadas. Esta función recibe los nuevos valores cálculados por parámetro y los compara con los valores cálculados en el último episodio (en el atributo `values`). Si la diferencia entre alguno de los valores nuevos y anteriores es superior al `threshold` definido para la clase, entonces actualizamos los valores de la clase y retornamos `False`. De lo contrario, la función retorna `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "681869fbcd2bf9e8fa5f373b70515300",
     "grade": true,
     "grade_id": "cell-4a372c49e9fb0330",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mcm = MCM(environment, discount=0.9, iterations=1000, threshold=0.00001)\n",
    "\n",
    "try:\n",
    "    mcm.check_convergence\n",
    "    assert type(mcm.check_convergence({'A': -10, 'B':3, 'C':-1, 'D':10, 'E':2.95})) is bool, \"La convergencia debe retornar un booleano\"\n",
    "except:\n",
    "    raise Exception(\"La función check_convergence no está definida\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56191f49928510af27314f5f09fbdc11",
     "grade": false,
     "grade_id": "cell-c9e54c575c6559eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 4. Cálculo de las recompensas\n",
    "\n",
    "Implemente la función `calculate_rewards` para un episodio (`episode`) dado por parámetro. Esta función debe tomar la secuencia de acciones (tuplas estado, accion, recompensa) del episodio y calcular la recompensa con descuento para cada uno de los estados. La función retorna un diccionario, con llave `estado` y valor lista de recompensas calculadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0fbc47aa38b9fc913dc8fe8e837c0a7",
     "grade": true,
     "grade_id": "cell-59553d47ff873131",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mcm = MCM(environment, discount=0.9, iterations=1000, threshold=0.00001)\n",
    "\n",
    "try:\n",
    "    mcm.calculate_rewards\n",
    "    episodes = mcm.generate_episode()\n",
    "    assert type(mcm.calculate_rewards(episodes)) is dict, \"La convergencia debe retornar un mapa con las recompensas para cada estado\"\n",
    "except:\n",
    "    raise Exception(\"La función calculate_rewards no está definida\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebas adicionales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7613c0848a1d6aafb6e71c1db4b8d69",
     "grade": false,
     "grade_id": "cell-e2a6989e290c0bce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 5.  Ejecución del agente\n",
    "\n",
    "Finalmente, debe ejecutar el agente utilizando el ambiente de la para observar los resultados obtenidos. La ejecución del agente ejecuta el método de Monte Carlo por el número de episodios con los que se crea el agente. Dentro de cada episodio el agente observa las muestras de la ejecución desde el estado inicial hasta un estado final. Basado en las muestras el agente toma las recompensas obtenidas en cada iteración y actualiza las recompensas y los valores. luego, basado en los valores de cada estado, se calcula la política correspondiente. Esta función retorna el mapa de valores y el mápa de la política resultado luego de la ejecución de todos los episodios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a372f4946c041084886231752689ff1",
     "grade": true,
     "grade_id": "cell-79a09b9dafb0687f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D': 10.0, 'C': 9.0, 'B': 5.987102445, 'E': 8.1}\n",
      "{'A': 'exit', 'C': 'down', 'E': 'up', 'B': 'right', 'D': 'exit'}\n"
     ]
    }
   ],
   "source": [
    "mcm = MCM(environment, discount=0.9, iterations=2, threshold=0.00001)\n",
    "\n",
    "values, policy = mcm.run()\n",
    "print(values)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
